多头注意力的基本思想是将模型的注意力分散到不同的表示子空间中，每个头**学习序列中不同的部分**。

给定查询 $\mathbf{q} \in \mathbb{R}^{d_q}$ 、键 $\mathbf{k} \in \mathbb{R}^{d_k}$和 值 $\mathbf{v} \in \mathbb{R}^{d_v}$ ，每个注意力头 $\mathbf{h}_i(i=1, \ldots, h)$ 的计算方法为：

$$
\mathbf{h}_i=f\left(\mathbf{W}_i^{(q)} \mathbf{q}, \mathbf{W}_i^{(k)} \mathbf{k}, \mathbf{W}_i^{(v)} \mathbf{v}\right) \in \mathbb{R}^{p_v},
$$

其中, 可学习的参数包括 $\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q} 、 \mathbf{W}_i^{(k)} \in \mathbb{R}^{p_k \times d_k}$ 和 $\mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d_v}$, 以及代表注意力汇聚的函数 $f$ 可以是加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换, 它对应着 $h$ 个头连结后的结果, 因此其可学习参数是 $\mathbf{W}_o \in \mathbb{R}^{p_o \times h p_v}$ :

$$
\mathbf{W}_o\left[\begin{array}{c}
\mathbf{h}_1 \\
\vdots \\
\mathbf{h}_h
\end{array}\right] \in \mathbb{R}^{p_o} .
$$

基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数，或者写成这样：

$$
\begin{gathered}
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\left[\text{head}_1, \ldots, \text { head }_h\right] \mathbf{W}_0 \\
\text { where head }_i=\text{Attention}\left(\mathbf{Q} \mathbf{W}_i^Q, \mathbf{K} \mathbf{W}_i^K, \mathbf{V} \mathbf{W}_i^V\right)
\end{gathered}
$$

<div align=center>
<img src="https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png" width="50%" height="50%" />
</div>

# 自己实现多头注意力机制







```python
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, query_size, key_size, value_size, num_hiddens, num_heads, dropout=0.0, bias=True):
        super(MultiHeadAttention, self).__init__()
        # num_hiddens：隐藏单元的数量。
        # num_heads：注意力头的数量。
        self.num_heads = num_heads
        self.head_dim = num_hiddens // num_heads

        assert num_hiddens % num_heads == 0, "num_hiddens must be divisible by num_heads"
        # 每个头可以得到一个大小相等的表示向量，确保隐藏层的维数可以平均分配到每个头上。
        self.scale = math.sqrt(self.head_dim)

        # 定义线性层
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

        # 正则化
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # Transform query, key, value
        # self.W_q(query)的意思是：
            # output = torch.matmul(query, self.W_q.weight.t())
            # if self.W_q.bias is not None:
            #     output += self.W_q.bias
        query = self.W_q(query).view(batch_size, -1, self.num_heads, self.head_dim)
        key = self.W_k(key).view(batch_size, -1, self.num_heads, self.head_dim)
        value = self.W_v(value).view(batch_size, -1, self.num_heads, self.head_dim)

        # 统一变化为 [batch_size, num_heads, seq_length, head_dim]
        query = query.transpose(1, 2)
        key = key.transpose(1, 2)
        value = value.transpose(1, 2)

        # 使用点积注意力机制计算分数，即 query 和 key 的转置乘积，然后除以缩放因子 self.scale（为了防止梯度消失或爆炸）。注意力分数表示 query 和 key 之间的相似性。
        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale

        if mask is not None:
            # [batch_size, 1, 1, seq_length] for broadcasting
            mask = mask.unsqueeze(1).unsqueeze(1)
            # 如果传入了 mask 参数，它会被重塑以适应注意力分数的形状，然后使用 masked_fill 方法将掩码为 0 的位置替换为负无穷。这通常用于屏蔽掉序列中的填充（padding）部分，以免影响注意力分数的计算。
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # 使用注意力权重对 value 进行加权求和，然后将不同头的输出进行合并
        context = torch.matmul(attention_weights, value)
        # [batch_size, -1, num_heads * head_dim]
        context = context.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.head_dim)

        # [batch_size, seq_length, num_hiddens]
        output = self.W_o(context)

        return output, attention_weights

# Example usage
query_size = 64
key_size = 128
value_size = 256
num_hiddens = 256
num_heads = 4
batch_size = 2
seq_length = 10

# Initialize the multi-head attention layer
multi_head_attn = MultiHeadAttention(query_size, key_size, value_size, num_hiddens, num_heads)

# Create dummy input tensors
query = torch.rand(batch_size, seq_length, query_size)
key = torch.rand(batch_size, seq_length, key_size)
value = torch.rand(batch_size, seq_length, value_size)

# Forward pass through the multi-head attention layer
output, attention_weights = multi_head_attn(query, key, value)

print("Output shape:", output.shape)  # Expected shape: (batch_size, seq_length, num_hiddens)
print("Attention weights shape:", attention_weights.shape)  # Expected shape: (batch_size, num_heads, seq_length, seq_length)


mask = torch.zeros(batch_size, seq_length, dtype=torch.bool)
mask[:, 4:] = 1  # 假设序列中从第5个位置开始是padding

# Forward pass through the multi-head attention layer with mask
output, attention_weights = multi_head_attn(query, key, value, mask=mask)

print("Mask output shape:", output.shape)  # Expected shape: (batch_size, seq_length, num_hiddens)
print("Mask attention weights shape:", attention_weights.shape)  # Expected shape: (batch_size, num_heads, seq_length, seq_length)

```

<!-- ##{"script":"<script>MathJax = {tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}};</script>"}## -->