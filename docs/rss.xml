<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Blog Title</title><link>https://Gmeekdai.github.io/test</link><description>Blog description</description><copyright>Blog Title</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://Gmeekdai.github.io/test</link></image><lastBuildDate>Tue, 02 Apr 2024 02:08:52 +0000</lastBuildDate><managingEditor>Blog Title</managingEditor><ttl>60</ttl><webMaster>Blog Title</webMaster><item><title>我是不合规的符号:符号/符号?符号*等等</title><link>https://Gmeekdai.github.io/test/post/wo-shi-bu-he-gui-de-fu-hao---fu-hao---fu-hao---fu-hao---deng-deng.html</link><description>我是不合规的符号:符号/符号?</description><guid isPermaLink="true">https://Gmeekdai.github.io/test/post/wo-shi-bu-he-gui-de-fu-hao---fu-hao---fu-hao---fu-hao---deng-deng.html</guid><pubDate>Tue, 02 Apr 2024 01:19:58 +0000</pubDate></item><item><title>解决行内公式显示不正常</title><link>https://Gmeekdai.github.io/test/post/jie-jue-xing-nei-gong-shi-xian-shi-bu-zheng-chang.html</link><description>多头注意力的基本思想是将模型的注意力分散到不同的表示子空间中，每个头**学习序列中不同的部分**。&#13;
&#13;
给定查询 $\mathbf{q} \in \mathbb{R}^{d_q}$ 、键 $\mathbf{k} \in \mathbb{R}^{d_k}$和 值 $\mathbf{v} \in \mathbb{R}^{d_v}$ ，每个注意力头 $\mathbf{h}_i(i=1, \ldots, h)$ 的计算方法为：&#13;
&#13;
$$&#13;
\mathbf{h}_i=f\left(\mathbf{W}_i^{(q)} \mathbf{q}, \mathbf{W}_i^{(k)} \mathbf{k}, \mathbf{W}_i^{(v)} \mathbf{v}\right) \in \mathbb{R}^{p_v},&#13;
$$&#13;
&#13;
其中, 可学习的参数包括 $\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q} 、 \mathbf{W}_i^{(k)} \in \mathbb{R}^{p_k \times d_k}$ 和 $\mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d_v}$, 以及代表注意力汇聚的函数 $f$ 可以是加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换, 它对应着 $h$ 个头连结后的结果, 因此其可学习参数是 $\mathbf{W}_o \in \mathbb{R}^{p_o \times h p_v}$ :&#13;
&#13;
$$&#13;
\mathbf{W}_o\left[\begin{array}{c}&#13;
\mathbf{h}_1 \\&#13;
\vdots \\&#13;
\mathbf{h}_h&#13;
\end{array}\right] \in \mathbb{R}^{p_o} .&#13;
$$&#13;
&#13;
基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数，或者写成这样：&#13;
&#13;
$$&#13;
\begin{gathered}&#13;
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\left[\text{head}_1, \ldots, \text { head }_h\right] \mathbf{W}_0 \\&#13;
\text { where head }_i=\text{Attention}\left(\mathbf{Q} \mathbf{W}_i^Q, \mathbf{K} \mathbf{W}_i^K, \mathbf{V} \mathbf{W}_i^V\right)&#13;
\end{gathered}&#13;
$$&#13;
&#13;
&lt;div align=center&gt;&#13;
&lt;img src="https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png" width="50%" height="50%" /&gt;&#13;
&lt;/div&gt;&#13;
&#13;
# 自己实现多头注意力机制&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
```python&#13;
import math&#13;
import torch&#13;
import torch.nn as nn&#13;
import torch.nn.functional as F&#13;
&#13;
class MultiHeadAttention(nn.Module):&#13;
    def __init__(self, query_size, key_size, value_size, num_hiddens, num_heads, dropout=0.0, bias=True):&#13;
        super(MultiHeadAttention, self).__init__()&#13;
        # num_hiddens：隐藏单元的数量。&#13;
        # num_heads：注意力头的数量。&#13;
        self.num_heads = num_heads&#13;
        self.head_dim = num_hiddens // num_heads&#13;
&#13;
        assert num_hiddens % num_heads == 0, "num_hiddens must be divisible by num_heads"&#13;
        # 每个头可以得到一个大小相等的表示向量，确保隐藏层的维数可以平均分配到每个头上。&#13;
        self.scale = math.sqrt(self.head_dim)&#13;
&#13;
        # 定义线性层&#13;
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)&#13;
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)&#13;
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)&#13;
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)&#13;
&#13;
        # 正则化&#13;
        self.dropout = nn.Dropout(dropout)&#13;
&#13;
    def forward(self, query, key, value, mask=None):&#13;
        batch_size = query.size(0)&#13;
&#13;
        # Transform query, key, value&#13;
        # self.W_q(query)的意思是：&#13;
            # output = torch.matmul(query, self.W_q.weight.t())&#13;
            # if self.W_q.bias is not None:&#13;
            #     output += self.W_q.bias&#13;
        query = self.W_q(query).view(batch_size, -1, self.num_heads, self.head_dim)&#13;
        key = self.W_k(key).view(batch_size, -1, self.num_heads, self.head_dim)&#13;
        value = self.W_v(value).view(batch_size, -1, self.num_heads, self.head_dim)&#13;
&#13;
        # 统一变化为 [batch_size, num_heads, seq_length, head_dim]&#13;
        query = query.transpose(1, 2)&#13;
        key = key.transpose(1, 2)&#13;
        value = value.transpose(1, 2)&#13;
&#13;
        # 使用点积注意力机制计算分数，即 query 和 key 的转置乘积，然后除以缩放因子 self.scale（为了防止梯度消失或爆炸）。注意力分数表示 query 和 key 之间的相似性。&#13;
        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale&#13;
&#13;
        if mask is not None:&#13;
            # [batch_size, 1, 1, seq_length] for broadcasting&#13;
            mask = mask.unsqueeze(1).unsqueeze(1)&#13;
            # 如果传入了 mask 参数，它会被重塑以适应注意力分数的形状，然后使用 masked_fill 方法将掩码为 0 的位置替换为负无穷。这通常用于屏蔽掉序列中的填充（padding）部分，以免影响注意力分数的计算。&#13;
            scores = scores.masked_fill(mask == 0, float('-inf'))&#13;
&#13;
        attention_weights = F.softmax(scores, dim=-1)&#13;
        attention_weights = self.dropout(attention_weights)&#13;
&#13;
        # 使用注意力权重对 value 进行加权求和，然后将不同头的输出进行合并&#13;
        context = torch.matmul(attention_weights, value)&#13;
        # [batch_size, -1, num_heads * head_dim]&#13;
        context = context.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.head_dim)&#13;
&#13;
        # [batch_size, seq_length, num_hiddens]&#13;
        output = self.W_o(context)&#13;
&#13;
        return output, attention_weights&#13;
&#13;
# Example usage&#13;
query_size = 64&#13;
key_size = 128&#13;
value_size = 256&#13;
num_hiddens = 256&#13;
num_heads = 4&#13;
batch_size = 2&#13;
seq_length = 10&#13;
&#13;
# Initialize the multi-head attention layer&#13;
multi_head_attn = MultiHeadAttention(query_size, key_size, value_size, num_hiddens, num_heads)&#13;
&#13;
# Create dummy input tensors&#13;
query = torch.rand(batch_size, seq_length, query_size)&#13;
key = torch.rand(batch_size, seq_length, key_size)&#13;
value = torch.rand(batch_size, seq_length, value_size)&#13;
&#13;
# Forward pass through the multi-head attention layer&#13;
output, attention_weights = multi_head_attn(query, key, value)&#13;
&#13;
print("Output shape:", output.shape)  # Expected shape: (batch_size, seq_length, num_hiddens)&#13;
print("Attention weights shape:", attention_weights.shape)  # Expected shape: (batch_size, num_heads, seq_length, seq_length)&#13;
&#13;
&#13;
mask = torch.zeros(batch_size, seq_length, dtype=torch.bool)&#13;
mask[:, 4:] = 1  # 假设序列中从第5个位置开始是padding&#13;
&#13;
# Forward pass through the multi-head attention layer with mask&#13;
output, attention_weights = multi_head_attn(query, key, value, mask=mask)&#13;
&#13;
print("Mask output shape:", output.shape)  # Expected shape: (batch_size, seq_length, num_hiddens)&#13;
print("Mask attention weights shape:", attention_weights.shape)  # Expected shape: (batch_size, num_heads, seq_length, seq_length)&#13;
&#13;
```&#13;
?</description><guid isPermaLink="true">https://Gmeekdai.github.io/test/post/jie-jue-xing-nei-gong-shi-xian-shi-bu-zheng-chang.html</guid><pubDate>Mon, 01 Apr 2024 05:56:48 +0000</pubDate></item><item><title>test</title><link>https://Gmeekdai.github.io/test/post/test.html</link><description>test&#13;
&#13;
&lt;!-- ##{"script":"&lt;script&gt;let post=0&lt;/script&gt;"}## --&gt;?</description><guid isPermaLink="true">https://Gmeekdai.github.io/test/post/test.html</guid><pubDate>Mon, 18 Mar 2024 09:45:35 +0000</pubDate></item><item><title>add comment</title><link>https://Gmeekdai.github.io/test/comment.html</link><description>add comment?</description><guid isPermaLink="true">https://Gmeekdai.github.io/test/comment.html</guid><pubDate>Mon, 25 Mar 2024 09:33:00 +0000</pubDate></item><item><title>关于我啊</title><link>https://Gmeekdai.github.io/test/about.html</link><description>关于我啊?</description><guid isPermaLink="true">https://Gmeekdai.github.io/test/about.html</guid><pubDate>Tue, 26 Mar 2024 02:39:51 +0000</pubDate></item></channel></rss>