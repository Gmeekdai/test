{"singlePage": ["comment", "about"], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "<script>let meekdai=0</script>", "style": "", "bottomText": "", "showPostSource": 1, "iconList": {"about": "M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13Z", "comment": "M2 7.75A.75.75 0 0 1 2.75 7h10a.75.75 0 0 1 0 1.5h-10A.75.75 0 0 1 2 7.75Z"}, "UTC": 8, "rssMode": "all", "title": "Blog Title", "subTitle": "Blog description", "avatarUrl": "https://github.githubassets.com/favicons/favicon.svg", "GMEEK_VERSION": "main", "postListJson": {"P1": {"htmlDir": "docs/post/test.html", "label": "good first issue", "labelColor": "#7057ff", "postTitle": "test", "postUrl": "post/test.html", "postSourceUrl": "https://github.com/Gmeekdai/test/issues/1", "commentNum": 0, "wordCount": 61, "description": "test\r\n\r\n<!-- ##{\"script\":\"<script>let post=0</script>\"}## -->", "top": 0, "createdAt": 1710755135, "style": "", "script": "<script>let meekdai=0</script><script>let post=0</script>", "createdDate": "2024-03-18", "dateLabelColor": "#bc4c00"}, "P4": {"htmlDir": "docs/post/jie-jue-xing-nei-gong-shi-xian-shi-bu-zheng-chang.html", "label": "bug", "labelColor": "#d73a4a", "postTitle": "\u89e3\u51b3\u884c\u5185\u516c\u5f0f\u663e\u793a\u4e0d\u6b63\u5e38", "postUrl": "post/jie-jue-xing-nei-gong-shi-xian-shi-bu-zheng-chang.html", "postSourceUrl": "https://github.com/Gmeekdai/test/issues/4", "commentNum": 0, "wordCount": 5406, "description": "\u591a\u5934\u6ce8\u610f\u529b\u7684\u57fa\u672c\u601d\u60f3\u662f\u5c06\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5206\u6563\u5230\u4e0d\u540c\u7684\u8868\u793a\u5b50\u7a7a\u95f4\u4e2d\uff0c\u6bcf\u4e2a\u5934**\u5b66\u4e60\u5e8f\u5217\u4e2d\u4e0d\u540c\u7684\u90e8\u5206**\u3002\r\n\r\n\u7ed9\u5b9a\u67e5\u8be2 $\\mathbf{q} \\in \\mathbb{R}^{d_q}$ \u3001\u952e $\\mathbf{k} \\in \\mathbb{R}^{d_k}$\u548c \u503c $\\mathbf{v} \\in \\mathbb{R}^{d_v}$ \uff0c\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934 $\\mathbf{h}_i(i=1, \\ldots, h)$ \u7684\u8ba1\u7b97\u65b9\u6cd5\u4e3a\uff1a\r\n\r\n$$\r\n\\mathbf{h}_i=f\\left(\\mathbf{W}_i^{(q)} \\mathbf{q}, \\mathbf{W}_i^{(k)} \\mathbf{k}, \\mathbf{W}_i^{(v)} \\mathbf{v}\\right) \\in \\mathbb{R}^{p_v},\r\n$$\r\n\r\n\u5176\u4e2d, \u53ef\u5b66\u4e60\u7684\u53c2\u6570\u5305\u62ec $\\mathbf{W}_i^{(q)} \\in \\mathbb{R}^{p_q \\times d_q} \u3001 \\mathbf{W}_i^{(k)} \\in \\mathbb{R}^{p_k \\times d_k}$ \u548c $\\mathbf{W}_i^{(v)} \\in \\mathbb{R}^{p_v \\times d_v}$, \u4ee5\u53ca\u4ee3\u8868\u6ce8\u610f\u529b\u6c47\u805a\u7684\u51fd\u6570 $f$ \u53ef\u4ee5\u662f\u52a0\u6027\u6ce8\u610f\u529b\u548c\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u3002\u591a\u5934\u6ce8\u610f\u529b\u7684\u8f93\u51fa\u9700\u8981\u7ecf\u8fc7\u53e6\u4e00\u4e2a\u7ebf\u6027\u8f6c\u6362, \u5b83\u5bf9\u5e94\u7740 $h$ \u4e2a\u5934\u8fde\u7ed3\u540e\u7684\u7ed3\u679c, \u56e0\u6b64\u5176\u53ef\u5b66\u4e60\u53c2\u6570\u662f $\\mathbf{W}_o \\in \\mathbb{R}^{p_o \\times h p_v}$ :\r\n\r\n$$\r\n\\mathbf{W}_o\\left[\\begin{array}{c}\r\n\\mathbf{h}_1 \\\\\r\n\\vdots \\\\\r\n\\mathbf{h}_h\r\n\\end{array}\\right] \\in \\mathbb{R}^{p_o} .\r\n$$\r\n\r\n\u57fa\u4e8e\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u6bcf\u4e2a\u5934\u90fd\u53ef\u80fd\u4f1a\u5173\u6ce8\u8f93\u5165\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u53ef\u4ee5\u8868\u793a\u6bd4\u7b80\u5355\u52a0\u6743\u5e73\u5747\u503c\u66f4\u590d\u6742\u7684\u51fd\u6570\uff0c\u6216\u8005\u5199\u6210\u8fd9\u6837\uff1a\r\n\r\n$$\r\n\\begin{gathered}\r\n\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\left[\\text{head}_1, \\ldots, \\text { head }_h\\right] \\mathbf{W}_0 \\\\\r\n\\text { where head }_i=\\text{Attention}\\left(\\mathbf{Q} \\mathbf{W}_i^Q, \\mathbf{K} \\mathbf{W}_i^K, \\mathbf{V} \\mathbf{W}_i^V\\right)\r\n\\end{gathered}\r\n$$\r\n\r\n<div align=center>\r\n<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" width=\"50%\" height=\"50%\" />\r\n</div>\r\n\r\n# \u81ea\u5df1\u5b9e\u73b0\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n```python\r\nimport math\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass MultiHeadAttention(nn.Module):\r\n    def __init__(self, query_size, key_size, value_size, num_hiddens, num_heads, dropout=0.0, bias=True):\r\n        super(MultiHeadAttention, self).__init__()\r\n        # num_hiddens\uff1a\u9690\u85cf\u5355\u5143\u7684\u6570\u91cf\u3002\r\n        # num_heads\uff1a\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf\u3002\r\n        self.num_heads = num_heads\r\n        self.head_dim = num_hiddens // num_heads\r\n\r\n        assert num_hiddens % num_heads == 0, \"num_hiddens must be divisible by num_heads\"\r\n        # \u6bcf\u4e2a\u5934\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u5927\u5c0f\u76f8\u7b49\u7684\u8868\u793a\u5411\u91cf\uff0c\u786e\u4fdd\u9690\u85cf\u5c42\u7684\u7ef4\u6570\u53ef\u4ee5\u5e73\u5747\u5206\u914d\u5230\u6bcf\u4e2a\u5934\u4e0a\u3002\r\n        self.scale = math.sqrt(self.head_dim)\r\n\r\n        # \u5b9a\u4e49\u7ebf\u6027\u5c42\r\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\r\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\r\n        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\r\n        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\r\n\r\n        # \u6b63\u5219\u5316\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, query, key, value, mask=None):\r\n        batch_size = query.size(0)\r\n\r\n        # Transform query, key, value\r\n        # self.W_q(query)\u7684\u610f\u601d\u662f\uff1a\r\n            # output = torch.matmul(query, self.W_q.weight.t())\r\n            # if self.W_q.bias is not None:\r\n            #     output += self.W_q.bias\r\n        query = self.W_q(query).view(batch_size, -1, self.num_heads, self.head_dim)\r\n        key = self.W_k(key).view(batch_size, -1, self.num_heads, self.head_dim)\r\n        value = self.W_v(value).view(batch_size, -1, self.num_heads, self.head_dim)\r\n\r\n        # \u7edf\u4e00\u53d8\u5316\u4e3a [batch_size, num_heads, seq_length, head_dim]\r\n        query = query.transpose(1, 2)\r\n        key = key.transpose(1, 2)\r\n        value = value.transpose(1, 2)\r\n\r\n        # \u4f7f\u7528\u70b9\u79ef\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u5206\u6570\uff0c\u5373 query \u548c key \u7684\u8f6c\u7f6e\u4e58\u79ef\uff0c\u7136\u540e\u9664\u4ee5\u7f29\u653e\u56e0\u5b50 self.scale\uff08\u4e3a\u4e86\u9632\u6b62\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\uff09\u3002\u6ce8\u610f\u529b\u5206\u6570\u8868\u793a query \u548c key \u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\r\n        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale\r\n\r\n        if mask is not None:\r\n            # [batch_size, 1, 1, seq_length] for broadcasting\r\n            mask = mask.unsqueeze(1).unsqueeze(1)\r\n            # \u5982\u679c\u4f20\u5165\u4e86 mask \u53c2\u6570\uff0c\u5b83\u4f1a\u88ab\u91cd\u5851\u4ee5\u9002\u5e94\u6ce8\u610f\u529b\u5206\u6570\u7684\u5f62\u72b6\uff0c\u7136\u540e\u4f7f\u7528 masked_fill \u65b9\u6cd5\u5c06\u63a9\u7801\u4e3a 0 \u7684\u4f4d\u7f6e\u66ff\u6362\u4e3a\u8d1f\u65e0\u7a77\u3002\u8fd9\u901a\u5e38\u7528\u4e8e\u5c4f\u853d\u6389\u5e8f\u5217\u4e2d\u7684\u586b\u5145\uff08padding\uff09\u90e8\u5206\uff0c\u4ee5\u514d\u5f71\u54cd\u6ce8\u610f\u529b\u5206\u6570\u7684\u8ba1\u7b97\u3002\r\n            scores = scores.masked_fill(mask == 0, float('-inf'))\r\n\r\n        attention_weights = F.softmax(scores, dim=-1)\r\n        attention_weights = self.dropout(attention_weights)\r\n\r\n        # \u4f7f\u7528\u6ce8\u610f\u529b\u6743\u91cd\u5bf9 value \u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7136\u540e\u5c06\u4e0d\u540c\u5934\u7684\u8f93\u51fa\u8fdb\u884c\u5408\u5e76\r\n        context = torch.matmul(attention_weights, value)\r\n        # [batch_size, -1, num_heads * head_dim]\r\n        context = context.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.head_dim)\r\n\r\n        # [batch_size, seq_length, num_hiddens]\r\n        output = self.W_o(context)\r\n\r\n        return output, attention_weights\r\n\r\n# Example usage\r\nquery_size = 64\r\nkey_size = 128\r\nvalue_size = 256\r\nnum_hiddens = 256\r\nnum_heads = 4\r\nbatch_size = 2\r\nseq_length = 10\r\n\r\n# Initialize the multi-head attention layer\r\nmulti_head_attn = MultiHeadAttention(query_size, key_size, value_size, num_hiddens, num_heads)\r\n\r\n# Create dummy input tensors\r\nquery = torch.rand(batch_size, seq_length, query_size)\r\nkey = torch.rand(batch_size, seq_length, key_size)\r\nvalue = torch.rand(batch_size, seq_length, value_size)\r\n\r\n# Forward pass through the multi-head attention layer\r\noutput, attention_weights = multi_head_attn(query, key, value)\r\n\r\nprint(\"Output shape:\", output.shape)  # Expected shape: (batch_size, seq_length, num_hiddens)\r\nprint(\"Attention weights shape:\", attention_weights.shape)  # Expected shape: (batch_size, num_heads, seq_length, seq_length)\r\n\r\n\r\nmask = torch.zeros(batch_size, seq_length, dtype=torch.bool)\r\nmask[:, 4:] = 1  # \u5047\u8bbe\u5e8f\u5217\u4e2d\u4ece\u7b2c5\u4e2a\u4f4d\u7f6e\u5f00\u59cb\u662fpadding\r\n\r\n# Forward pass through the multi-head attention layer with mask\r\noutput, attention_weights = multi_head_attn(query, key, value, mask=mask)\r\n\r\nprint(\"Mask output shape:\", output.shape)  # Expected shape: (batch_size, seq_length, num_hiddens)\r\nprint(\"Mask attention weights shape:\", attention_weights.shape)  # Expected shape: (batch_size, num_heads, seq_length, seq_length)\r\n\r\n```\r\n", "top": 0, "createdAt": 1711951008, "style": "", "script": "<script>let meekdai=0</script><script>MathJax = {tex: {inlineMath: [[\"$\", \"$\"], [\"\\(\", \"\\)\"]]}};</script><script async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>", "createdDate": "2024-04-01", "dateLabelColor": "#bc4c00"}}, "singeListJson": {"P3": {"htmlDir": "docs/about.html", "label": "about", "labelColor": "#bfd4f2", "postTitle": "\u5173\u4e8e\u6211\u554a", "postUrl": "about.html", "postSourceUrl": "https://github.com/Gmeekdai/test/issues/3", "commentNum": 0, "wordCount": 4, "description": "\u5173\u4e8e\u6211\u554a", "top": 0, "createdAt": 1711420791, "style": "", "script": "<script>let meekdai=0</script>", "createdDate": "2024-03-26", "dateLabelColor": "#bc4c00"}, "P2": {"htmlDir": "docs/comment.html", "label": "comment", "labelColor": "#fef2c0", "postTitle": "add comment", "postUrl": "comment.html", "postSourceUrl": "https://github.com/Gmeekdai/test/issues/2", "commentNum": 0, "wordCount": 11, "description": "add comment", "top": 0, "createdAt": 1711359180, "style": "", "script": "<script>let meekdai=0</script>", "createdDate": "2024-03-25", "dateLabelColor": "#bc4c00"}}, "displayTitle": "Blog Title", "faviconUrl": "https://github.githubassets.com/favicons/favicon.svg", "homeUrl": "https://Gmeekdai.github.io/test", "prevUrl": "disabled", "nextUrl": "disabled"}